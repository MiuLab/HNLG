# no repeat
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_cur_norepeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_cur_norepeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_cur_norepeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_cur_norepeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_cur_norepeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_cur_norepeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_cur_norepeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_cur_norepeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_cur_norepeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_cur_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_cur_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_cur_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_3
# seq2seq
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 1 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 400 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --h_attn 0 --dir_name seq2seq_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 1 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 400 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --h_attn 0 --dir_name seq2seq_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 1 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 400 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --h_attn 0 --dir_name seq2seq_3
# no repeat no cur
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_norepeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_norepeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_norepeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_norepeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_norepeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_norepeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_norepeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_norepeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 0 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_norepeat_3
# repeat cur
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_cur_repeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_cur_repeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_dot --h_attn 1 --dir_name h_dot_cur_repeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_cur_repeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_cur_repeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_concat2 --h_attn 1 --dir_name h_concat2_cur_repeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_cur_repeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_cur_repeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method h_general --h_attn 1 --dir_name h_general_cur_repeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_cur_repeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_cur_repeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 1 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_cur_repeat_3
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_repeat_1
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_repeat_2
python train.py --data_dir ../data/ --dataset E2ENLG --fold_attr 1 --vocab_size 500 --use_embedding 0 --regen 0 --replace_model 0 --is_spacy 1 --is_lemma 1 --use_punct 0 --en_max_length -1 --de_max_length -1 --min_length 5 --partition_ratio 0.95 --cell GRU --n_layers 4 --n_en_layers 1 --n_de_layers 1 --en_hidden_size 200 --de_hidden_size 100 --en_embedding 0 --en_use_attr_init_state 1 --share_embedding 0 --embedding_dim 50 --en_embedding_dim 50 --de_embedding_dim 50 --attn_method none --bidirectional 1 --feed_last 1 --repeat_input 1 --batch_norm 0 --epochs 20 --batch_size 32 --en_optimizer Adam --de_optimizer Adam --en_learning_rate 0.001 --de_learning_rate 0.001 --split_teacher_forcing 1 --teacher_forcing_ratio 0.5 --inner_teacher_forcing_ratio 0.5 --inter_teacher_forcing_ratio 0.5 --tf_decay_rate 0.9 --inner_tf_decay_rate 0.9 --inter_tf_decay_rate 0.9 --schedule_sampling 0 --inner_schedule_sampling 1 --inter_schedule_sampling 1 --is_curriculum 0 --padding_loss 0.0 --eos_loss 1.0 --max_norm 0.25 --finetune_embedding 0 --verbose_level 1 --verbose_epochs 0 --verbose_batches 500 --valid_epochs 1 --valid_batches 20 --save_epochs 1 --is_load 0 --check_mem_usage_batches 0 --attn_method none --dir_name hd_repeat_3
